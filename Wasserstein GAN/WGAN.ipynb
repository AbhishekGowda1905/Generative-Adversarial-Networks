{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Wasserstein GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sTpFE_eSk4s"
      },
      "source": [
        "#### Packages and Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def make_grad_hook():\n",
        "    grads = []\n",
        "    def grad_hook(m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            grads.append(m.weight.grad)\n",
        "    return grads, grad_hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A1M6kpnfxw"
      },
      "source": [
        "#### Generator and Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFwajQ3tGgI2"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            self.make_gen_block(z_dim, hidden_dim * 4),\n",
        "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
        "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
        "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
        "        \n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        \n",
        "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
        "        return self.gen(x)\n",
        "\n",
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    return torch.randn(n_samples, z_dim, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9fScH98nkYH"
      },
      "source": [
        "#### Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA4AxGnmpuPq"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    \n",
        "    def __init__(self, im_chan=1, hidden_dim=64):\n",
        "        super(Critic, self).__init__()\n",
        "        self.crit = nn.Sequential(\n",
        "            self.make_crit_block(im_chan, hidden_dim),\n",
        "            self.make_crit_block(hidden_dim, hidden_dim * 2),\n",
        "            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n",
        "        )\n",
        "\n",
        "    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
        "        \n",
        "        if not final_layer:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "                nn.BatchNorm2d(output_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
        "            )\n",
        "\n",
        "    def forward(self, image):\n",
        "        crit_pred = self.crit(image)\n",
        "        return crit_pred.view(len(crit_pred), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFLQ039u-qdu"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "z_dim = 64\n",
        "display_step = 50\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "beta_2 = 0.999\n",
        "c_lambda = 10\n",
        "crit_repeats = 5\n",
        "device = 'cuda'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    MNIST('.', download=True, transform=transform),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDFRZ8tg_Y57"
      },
      "outputs": [],
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "crit = Critic().to(device) \n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "gen = gen.apply(weights_init)\n",
        "crit = crit.apply(weights_init)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn4dkXnNtcv6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_gradient(crit, real, fake, epsilon):\n",
        "    \n",
        "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
        "    mixed_scores = crit(mixed_images)\n",
        "    \n",
        "    \n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=mixed_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores), \n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    return gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8av4TtbMtkTq"
      },
      "outputs": [],
      "source": [
        "def test_get_gradient(image_shape):\n",
        "    real = torch.randn(*image_shape, device=device) + 1\n",
        "    fake = torch.randn(*image_shape, device=device) - 1\n",
        "    epsilon_shape = [1 for _ in image_shape]\n",
        "    epsilon_shape[0] = image_shape[0]\n",
        "    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()\n",
        "    gradient = get_gradient(crit, real, fake, epsilon)\n",
        "    assert tuple(gradient.shape) == image_shape\n",
        "    assert gradient.max() > 0\n",
        "    assert gradient.min() < 0\n",
        "    return gradient\n",
        "\n",
        "gradient = test_get_gradient((256, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPwBH83IzCpS"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(gradient):\n",
        "    \n",
        "    gradient = gradient.view(len(gradient), -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    penalty = torch.mean((gradient_norm - 1)**2)\n",
        "  \n",
        "    return penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahPfGMA2zABQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_gradient_penalty(image_shape):\n",
        "    bad_gradient = torch.zeros(*image_shape)\n",
        "    bad_gradient_penalty = gradient_penalty(bad_gradient)\n",
        "    assert torch.isclose(bad_gradient_penalty, torch.tensor(1.))\n",
        "\n",
        "    image_size = torch.prod(torch.Tensor(image_shape[1:]))\n",
        "    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)\n",
        "    good_gradient_penalty = gradient_penalty(good_gradient)\n",
        "    assert torch.isclose(good_gradient_penalty, torch.tensor(0.))\n",
        "\n",
        "    random_gradient = test_get_gradient(image_shape)\n",
        "    random_gradient_penalty = gradient_penalty(random_gradient)\n",
        "    assert torch.abs(random_gradient_penalty - 1) < 0.1\n",
        "\n",
        "test_gradient_penalty((256, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnJFs-qkMCA-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_gen_loss(crit_fake_pred):\n",
        "    \n",
        "    gen_loss = -1. * torch.mean(crit_fake_pred)\n",
        "    return gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYVqG8bR6Hfg"
      },
      "outputs": [],
      "source": [
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.tensor(1.)), torch.tensor(-1.0)\n",
        ")\n",
        "\n",
        "assert torch.isclose(\n",
        "    get_gen_loss(torch.rand(10000)), torch.tensor(-0.5), 0.05\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jvbz1zDMTdu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
        "   \n",
        "    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp\n",
        "    return crit_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxZey6fc5luf"
      },
      "outputs": [],
      "source": [
        "\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(1.), torch.tensor(2.), torch.tensor(3.), 0.1),\n",
        "    torch.tensor(-0.7)\n",
        ")\n",
        "assert torch.isclose(\n",
        "    get_crit_loss(torch.tensor(20.), torch.tensor(-20.), torch.tensor(2.), 10),\n",
        "    torch.tensor(60.)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXptQZcwrBrq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cur_step = 0\n",
        "generator_losses = []\n",
        "critic_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    for real, _ in tqdm(dataloader):\n",
        "        cur_batch_size = len(real)\n",
        "        real = real.to(device)\n",
        "\n",
        "        mean_iteration_critic_loss = 0\n",
        "        for _ in range(crit_repeats):\n",
        "            #Update critic\n",
        "            crit_opt.zero_grad()\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "            fake = gen(fake_noise)\n",
        "            crit_fake_pred = crit(fake.detach())\n",
        "            crit_real_pred = crit(real)\n",
        "\n",
        "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
        "            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n",
        "            gp = gradient_penalty(gradient)\n",
        "            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
        "\n",
        "            # track the average critic loss in the batch\n",
        "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
        "            # Update gradients\n",
        "            crit_loss.backward(retain_graph=True)\n",
        "            # Update optimizer\n",
        "            crit_opt.step()\n",
        "        critic_losses += [mean_iteration_critic_loss]\n",
        "\n",
        "        #generator update\n",
        "        gen_opt.zero_grad()\n",
        "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
        "        fake_2 = gen(fake_noise_2)\n",
        "        crit_fake_pred = crit(fake_2)\n",
        "        \n",
        "        gen_loss = get_gen_loss(crit_fake_pred)\n",
        "        gen_loss.backward()\n",
        "\n",
        "        # weights update\n",
        "        gen_opt.step()\n",
        "\n",
        "        # tracking the average generator loss\n",
        "        generator_losses += [gen_loss.item()]\n",
        "\n",
        "        #Visualization code\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
        "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
        "            print(f\"Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
        "            \n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            step_bins = 20\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\"\n",
        "            )\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Critic Loss\"\n",
        "            )\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        cur_step += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcSEkItqC_25"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import zipfile\n",
        "from abc import ABC, abstractmethod\n",
        "from contextlib import contextmanager\n",
        "from functools import partial\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from typing import Iterable, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow.compat.v1 as tf\n",
        "from scipy import linalg\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INCEPTION_V3_URL = \"https://openaipublic.blob.core.windows.net/diffusion/jul-2021/ref_batches/classify_image_graph_def.pb\"\n",
        "INCEPTION_V3_PATH = \"classify_image_graph_def.pb\"\n",
        "\n",
        "\n",
        "FID_POOL_NAME = \"pool_3:0\"\n",
        "FID_SPATIAL_NAME = \"mixed_6/conv:0\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"ref_batch\", help=\"path to reference batch npz file\")\n",
        "    parser.add_argument(\"sample_batch\", help=\"path to sample batch npz file\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    config = tf.ConfigProto(\n",
        "        allow_soft_placement=True  # allows DecodeJpeg to run on CPU in Inception graph\n",
        "    )\n",
        "    config.gpu_options.allow_growth = True\n",
        "    evaluator = Evaluator(tf.Session(config=config))\n",
        "\n",
        "    print(\"warming up TensorFlow...\")\n",
        "    # This will cause TF to print a bunch of verbose stuff now rather\n",
        "    # than after the next print(), to help prevent confusion.\n",
        "    evaluator.warmup()\n",
        "\n",
        "    print(\"computing reference batch activations...\")\n",
        "    ref_acts = evaluator.read_activations(args.ref_batch)\n",
        "    print(\"computing/reading reference batch statistics...\")\n",
        "    ref_stats, ref_stats_spatial = evaluator.read_statistics(args.ref_batch, ref_acts)\n",
        "\n",
        "    print(\"computing sample batch activations...\")\n",
        "    sample_acts = evaluator.read_activations(args.sample_batch)\n",
        "    print(\"computing/reading sample batch statistics...\")\n",
        "    sample_stats, sample_stats_spatial = evaluator.read_statistics(args.sample_batch, sample_acts)\n",
        "\n",
        "    print(\"Computing evaluations...\")\n",
        "    print(\"Inception Score:\", evaluator.compute_inception_score(sample_acts[0]))\n",
        "    print(\"FID:\", sample_stats.frechet_distance(ref_stats))\n",
        "    print(\"sFID:\", sample_stats_spatial.frechet_distance(ref_stats_spatial))\n",
        "    prec, recall = evaluator.compute_prec_recall(ref_acts[0], sample_acts[0])\n",
        "    print(\"Precision:\", prec)\n",
        "    print(\"Recall:\", recall)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mt6C1ZnXIdb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HsuZlzWKcBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvalidFIDException(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class FIDStatistics:\n",
        "    def __init__(self, mu: np.ndarray, sigma: np.ndarray):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def frechet_distance(self, other, eps=1e-6):\n",
        "        \"\"\"\n",
        "        Compute the Frechet distance between two sets of statistics.\n",
        "        \"\"\"\n",
        "        # https://github.com/bioinf-jku/TTUR/blob/73ab375cdf952a12686d9aa7978567771084da42/fid.py#L132\n",
        "        mu1, sigma1 = self.mu, self.sigma\n",
        "        mu2, sigma2 = other.mu, other.sigma\n",
        "\n",
        "        mu1 = np.atleast_1d(mu1)\n",
        "        mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "        sigma1 = np.atleast_2d(sigma1)\n",
        "        sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "        assert (\n",
        "            mu1.shape == mu2.shape\n",
        "        ), f\"Training and test mean vectors have different lengths: {mu1.shape}, {mu2.shape}\"\n",
        "        assert (\n",
        "            sigma1.shape == sigma2.shape\n",
        "        ), f\"Training and test covariances have different dimensions: {sigma1.shape}, {sigma2.shape}\"\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "\n",
        "        # product might be almost singular\n",
        "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "        if not np.isfinite(covmean).all():\n",
        "            msg = (\n",
        "                \"fid calculation produces singular product; adding %s to diagonal of cov estimates\"\n",
        "                % eps\n",
        "            )\n",
        "            warnings.warn(msg)\n",
        "            offset = np.eye(sigma1.shape[0]) * eps\n",
        "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "        # numerical error might give slight imaginary component\n",
        "        if np.iscomplexobj(covmean):\n",
        "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "                m = np.max(np.abs(covmean.imag))\n",
        "                raise ValueError(\"Imaginary component {}\".format(m))\n",
        "            covmean = covmean.real\n",
        "\n",
        "        tr_covmean = np.trace(covmean)\n",
        "\n",
        "        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OthILuh6HN8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        session,\n",
        "        batch_size=64,\n",
        "        softmax_batch_size=512,\n",
        "    ):\n",
        "        self.sess = session\n",
        "        self.batch_size = batch_size\n",
        "        self.softmax_batch_size = softmax_batch_size\n",
        "        self.manifold_estimator = ManifoldEstimator(session)\n",
        "        with self.sess.graph.as_default():\n",
        "            self.image_input = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n",
        "            self.softmax_input = tf.placeholder(tf.float32, shape=[None, 2048])\n",
        "            self.pool_features, self.spatial_features = _create_feature_graph(self.image_input)\n",
        "            self.softmax = _create_softmax_graph(self.softmax_input)\n",
        "\n",
        "    def warmup(self):\n",
        "        self.compute_activations(np.zeros([1, 8, 64, 64, 3]))\n",
        "\n",
        "    def read_activations(self, npz_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        with open_npz_array(npz_path, \"arr_0\") as reader:\n",
        "            return self.compute_activations(reader.read_batches(self.batch_size))\n",
        "\n",
        "    def compute_activations(self, batches: Iterable[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute image features for downstream evals.\n",
        "        :param batches: a iterator over NHWC numpy arrays in [0, 255].\n",
        "        :return: a tuple of numpy arrays of shape [N x X], where X is a feature\n",
        "                 dimension. The tuple is (pool_3, spatial).\n",
        "        \"\"\"\n",
        "        preds = []\n",
        "        spatial_preds = []\n",
        "        for batch in tqdm(batches):\n",
        "            batch = batch.astype(np.float32)\n",
        "            pred, spatial_pred = self.sess.run(\n",
        "                [self.pool_features, self.spatial_features], {self.image_input: batch}\n",
        "            )\n",
        "            preds.append(pred.reshape([pred.shape[0], -1]))\n",
        "            spatial_preds.append(spatial_pred.reshape([spatial_pred.shape[0], -1]))\n",
        "        return (\n",
        "            np.concatenate(preds, axis=0),\n",
        "            np.concatenate(spatial_preds, axis=0),\n",
        "        )\n",
        "\n",
        "    def read_statistics(\n",
        "        self, npz_path: str, activations: Tuple[np.ndarray, np.ndarray]\n",
        "    ) -> Tuple[FIDStatistics, FIDStatistics]:\n",
        "        obj = np.load(npz_path)\n",
        "        if \"mu\" in list(obj.keys()):\n",
        "            return FIDStatistics(obj[\"mu\"], obj[\"sigma\"]), FIDStatistics(\n",
        "                obj[\"mu_s\"], obj[\"sigma_s\"]\n",
        "            )\n",
        "        return tuple(self.compute_statistics(x) for x in activations)\n",
        "\n",
        "    def compute_statistics(self, activations: np.ndarray) -> FIDStatistics:\n",
        "        mu = np.mean(activations, axis=0)\n",
        "        sigma = np.cov(activations, rowvar=False)\n",
        "        return FIDStatistics(mu, sigma)\n",
        "\n",
        "    def compute_inception_score(self, activations: np.ndarray, split_size: int = 5000) -> float:\n",
        "        softmax_out = []\n",
        "        for i in range(0, len(activations), self.softmax_batch_size):\n",
        "            acts = activations[i : i + self.softmax_batch_size]\n",
        "            softmax_out.append(self.sess.run(self.softmax, feed_dict={self.softmax_input: acts}))\n",
        "        preds = np.concatenate(softmax_out, axis=0)\n",
        "        # https://github.com/openai/improved-gan/blob/4f5d1ec5c16a7eceb206f42bfc652693601e1d5c/inception_score/model.py#L46\n",
        "        scores = []\n",
        "        for i in range(0, len(preds), split_size):\n",
        "            part = preds[i : i + split_size]\n",
        "            kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "            kl = np.mean(np.sum(kl, 1))\n",
        "            scores.append(np.exp(kl))\n",
        "        return float(np.mean(scores))\n",
        "\n",
        "    def compute_prec_recall(\n",
        "        self, activations_ref: np.ndarray, activations_sample: np.ndarray\n",
        "    ) -> Tuple[float, float]:\n",
        "        radii_1 = self.manifold_estimator.manifold_radii(activations_ref)\n",
        "        radii_2 = self.manifold_estimator.manifold_radii(activations_sample)\n",
        "        pr = self.manifold_estimator.evaluate_pr(\n",
        "            activations_ref, radii_1, activations_sample, radii_2\n",
        "        )\n",
        "        return (float(pr[0][0]), float(pr[1][0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VA5hhoRWI0sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ManifoldEstimator:\n",
        "    \"\"\"\n",
        "    A helper for comparing manifolds of feature vectors.\n",
        "    Adapted from https://github.com/kynkaat/improved-precision-and-recall-metric/blob/f60f25e5ad933a79135c783fcda53de30f42c9b9/precision_recall.py#L57\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        session,\n",
        "        row_batch_size=10000,\n",
        "        col_batch_size=10000,\n",
        "        nhood_sizes=(3,),\n",
        "        clamp_to_percentile=None,\n",
        "        eps=1e-5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Estimate the manifold of given feature vectors.\n",
        "        :param session: the TensorFlow session.\n",
        "        :param row_batch_size: row batch size to compute pairwise distances\n",
        "                               (parameter to trade-off between memory usage and performance).\n",
        "        :param col_batch_size: column batch size to compute pairwise distances.\n",
        "        :param nhood_sizes: number of neighbors used to estimate the manifold.\n",
        "        :param clamp_to_percentile: prune hyperspheres that have radius larger than\n",
        "                                    the given percentile.\n",
        "        :param eps: small number for numerical stability.\n",
        "        \"\"\"\n",
        "        self.distance_block = DistanceBlock(session)\n",
        "        self.row_batch_size = row_batch_size\n",
        "        self.col_batch_size = col_batch_size\n",
        "        self.nhood_sizes = nhood_sizes\n",
        "        self.num_nhoods = len(nhood_sizes)\n",
        "        self.clamp_to_percentile = clamp_to_percentile\n",
        "        self.eps = eps\n",
        "\n",
        "    def warmup(self):\n",
        "        feats, radii = (\n",
        "            np.zeros([1, 2048], dtype=np.float32),\n",
        "            np.zeros([1, 1], dtype=np.float32),\n",
        "        )\n",
        "        self.evaluate_pr(feats, radii, feats, radii)\n",
        "\n",
        "    def manifold_radii(self, features: np.ndarray) -> np.ndarray:\n",
        "        num_images = len(features)\n",
        "\n",
        "        # Estimate manifold of features by calculating distances to k-NN of each sample.\n",
        "        radii = np.zeros([num_images, self.num_nhoods], dtype=np.float32)\n",
        "        distance_batch = np.zeros([self.row_batch_size, num_images], dtype=np.float32)\n",
        "        seq = np.arange(max(self.nhood_sizes) + 1, dtype=np.int32)\n",
        "\n",
        "        for begin1 in range(0, num_images, self.row_batch_size):\n",
        "            end1 = min(begin1 + self.row_batch_size, num_images)\n",
        "            row_batch = features[begin1:end1]\n",
        "\n",
        "            for begin2 in range(0, num_images, self.col_batch_size):\n",
        "                end2 = min(begin2 + self.col_batch_size, num_images)\n",
        "                col_batch = features[begin2:end2]\n",
        "\n",
        "                # Compute distances between batches.\n",
        "                distance_batch[\n",
        "                    0 : end1 - begin1, begin2:end2\n",
        "                ] = self.distance_block.pairwise_distances(row_batch, col_batch)\n",
        "\n",
        "            # Find the k-nearest neighbor from the current batch.\n",
        "            radii[begin1:end1, :] = np.concatenate(\n",
        "                [\n",
        "                    x[:, self.nhood_sizes]\n",
        "                    for x in _numpy_partition(distance_batch[0 : end1 - begin1, :], seq, axis=1)\n",
        "                ],\n",
        "                axis=0,\n",
        "            )\n",
        "\n",
        "        if self.clamp_to_percentile is not None:\n",
        "            max_distances = np.percentile(radii, self.clamp_to_percentile, axis=0)\n",
        "            radii[radii > max_distances] = 0\n",
        "        return radii\n",
        "\n",
        "    def evaluate(self, features: np.ndarray, radii: np.ndarray, eval_features: np.ndarray):\n",
        "        \"\"\"\n",
        "        Evaluate if new feature vectors are at the manifold.\n",
        "        \"\"\"\n",
        "        num_eval_images = eval_features.shape[0]\n",
        "        num_ref_images = radii.shape[0]\n",
        "        distance_batch = np.zeros([self.row_batch_size, num_ref_images], dtype=np.float32)\n",
        "        batch_predictions = np.zeros([num_eval_images, self.num_nhoods], dtype=np.int32)\n",
        "        max_realism_score = np.zeros([num_eval_images], dtype=np.float32)\n",
        "        nearest_indices = np.zeros([num_eval_images], dtype=np.int32)\n",
        "\n",
        "        for begin1 in range(0, num_eval_images, self.row_batch_size):\n",
        "            end1 = min(begin1 + self.row_batch_size, num_eval_images)\n",
        "            feature_batch = eval_features[begin1:end1]\n",
        "\n",
        "            for begin2 in range(0, num_ref_images, self.col_batch_size):\n",
        "                end2 = min(begin2 + self.col_batch_size, num_ref_images)\n",
        "                ref_batch = features[begin2:end2]\n",
        "\n",
        "                distance_batch[\n",
        "                    0 : end1 - begin1, begin2:end2\n",
        "                ] = self.distance_block.pairwise_distances(feature_batch, ref_batch)\n",
        "\n",
        "            # From the minibatch of new feature vectors, determine if they are in the estimated manifold.\n",
        "            # If a feature vector is inside a hypersphere of some reference sample, then\n",
        "            # the new sample lies at the estimated manifold.\n",
        "            # The radii of the hyperspheres are determined from distances of neighborhood size k.\n",
        "            samples_in_manifold = distance_batch[0 : end1 - begin1, :, None] <= radii\n",
        "            batch_predictions[begin1:end1] = np.any(samples_in_manifold, axis=1).astype(np.int32)\n",
        "\n",
        "            max_realism_score[begin1:end1] = np.max(\n",
        "                radii[:, 0] / (distance_batch[0 : end1 - begin1, :] + self.eps), axis=1\n",
        "            )\n",
        "            nearest_indices[begin1:end1] = np.argmin(distance_batch[0 : end1 - begin1, :], axis=1)\n",
        "\n",
        "        return {\n",
        "            \"fraction\": float(np.mean(batch_predictions)),\n",
        "            \"batch_predictions\": batch_predictions,\n",
        "            \"max_realisim_score\": max_realism_score,\n",
        "            \"nearest_indices\": nearest_indices,\n",
        "        }\n",
        "\n",
        "    def evaluate_pr(\n",
        "        self,\n",
        "        features_1: np.ndarray,\n",
        "        radii_1: np.ndarray,\n",
        "        features_2: np.ndarray,\n",
        "        radii_2: np.ndarray,\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Evaluate precision and recall efficiently.\n",
        "        :param features_1: [N1 x D] feature vectors for reference batch.\n",
        "        :param radii_1: [N1 x K1] radii for reference vectors.\n",
        "        :param features_2: [N2 x D] feature vectors for the other batch.\n",
        "        :param radii_2: [N x K2] radii for other vectors.\n",
        "        :return: a tuple of arrays for (precision, recall):\n",
        "                 - precision: an np.ndarray of length K1\n",
        "                 - recall: an np.ndarray of length K2\n",
        "        \"\"\"\n",
        "        features_1_status = np.zeros([len(features_1), radii_2.shape[1]], dtype=np.bool)\n",
        "        features_2_status = np.zeros([len(features_2), radii_1.shape[1]], dtype=np.bool)\n",
        "        for begin_1 in range(0, len(features_1), self.row_batch_size):\n",
        "            end_1 = begin_1 + self.row_batch_size\n",
        "            batch_1 = features_1[begin_1:end_1]\n",
        "            for begin_2 in range(0, len(features_2), self.col_batch_size):\n",
        "                end_2 = begin_2 + self.col_batch_size\n",
        "                batch_2 = features_2[begin_2:end_2]\n",
        "                batch_1_in, batch_2_in = self.distance_block.less_thans(\n",
        "                    batch_1, radii_1[begin_1:end_1], batch_2, radii_2[begin_2:end_2]\n",
        "                )\n",
        "                features_1_status[begin_1:end_1] |= batch_1_in\n",
        "                features_2_status[begin_2:end_2] |= batch_2_in\n",
        "        return (\n",
        "            np.mean(features_2_status.astype(np.float64), axis=0),\n",
        "            np.mean(features_1_status.astype(np.float64), axis=0),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "HCH9466hI0vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistanceBlock:\n",
        "    \"\"\"\n",
        "    Calculate pairwise distances between vectors.\n",
        "    Adapted from https://github.com/kynkaat/improved-precision-and-recall-metric/blob/f60f25e5ad933a79135c783fcda53de30f42c9b9/precision_recall.py#L34\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, session):\n",
        "        self.session = session\n",
        "\n",
        "        # Initialize TF graph to calculate pairwise distances.\n",
        "        with session.graph.as_default():\n",
        "            self._features_batch1 = tf.placeholder(tf.float32, shape=[None, None])\n",
        "            self._features_batch2 = tf.placeholder(tf.float32, shape=[None, None])\n",
        "            distance_block_16 = _batch_pairwise_distances(\n",
        "                tf.cast(self._features_batch1, tf.float16),\n",
        "                tf.cast(self._features_batch2, tf.float16),\n",
        "            )\n",
        "            self.distance_block = tf.cond(\n",
        "                tf.reduce_all(tf.math.is_finite(distance_block_16)),\n",
        "                lambda: tf.cast(distance_block_16, tf.float32),\n",
        "                lambda: _batch_pairwise_distances(self._features_batch1, self._features_batch2),\n",
        "            )\n",
        "\n",
        "            # Extra logic for less thans.\n",
        "            self._radii1 = tf.placeholder(tf.float32, shape=[None, None])\n",
        "            self._radii2 = tf.placeholder(tf.float32, shape=[None, None])\n",
        "            dist32 = tf.cast(self.distance_block, tf.float32)[..., None]\n",
        "            self._batch_1_in = tf.math.reduce_any(dist32 <= self._radii2, axis=1)\n",
        "            self._batch_2_in = tf.math.reduce_any(dist32 <= self._radii1[:, None], axis=0)\n",
        "\n",
        "    def pairwise_distances(self, U, V):\n",
        "        \"\"\"\n",
        "        Evaluate pairwise distances between two batches of feature vectors.\n",
        "        \"\"\"\n",
        "        return self.session.run(\n",
        "            self.distance_block,\n",
        "            feed_dict={self._features_batch1: U, self._features_batch2: V},\n",
        "        )\n",
        "\n",
        "    def less_thans(self, batch_1, radii_1, batch_2, radii_2):\n",
        "        return self.session.run(\n",
        "            [self._batch_1_in, self._batch_2_in],\n",
        "            feed_dict={\n",
        "                self._features_batch1: batch_1,\n",
        "                self._features_batch2: batch_2,\n",
        "                self._radii1: radii_1,\n",
        "                self._radii2: radii_2,\n",
        "            },\n",
        "        )\n",
        "\n",
        "\n",
        "def _batch_pairwise_distances(U, V):\n",
        "    \"\"\"\n",
        "    Compute pairwise distances between two batches of feature vectors.\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(\"pairwise_dist_block\"):\n",
        "        # Squared norms of each row in U and V.\n",
        "        norm_u = tf.reduce_sum(tf.square(U), 1)\n",
        "        norm_v = tf.reduce_sum(tf.square(V), 1)\n",
        "\n",
        "        # norm_u as a column and norm_v as a row vectors.\n",
        "        norm_u = tf.reshape(norm_u, [-1, 1])\n",
        "        norm_v = tf.reshape(norm_v, [1, -1])\n",
        "\n",
        "        # Pairwise squared Euclidean distances.\n",
        "        D = tf.maximum(norm_u - 2 * tf.matmul(U, V, False, True) + norm_v, 0.0)\n",
        "\n",
        "    return D\n"
      ],
      "metadata": {
        "id": "uwyjPQ21I0x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NpzArrayReader(ABC):\n",
        "    @abstractmethod\n",
        "    def read_batch(self, batch_size: int) -> Optional[np.ndarray]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def remaining(self) -> int:\n",
        "        pass\n",
        "\n",
        "    def read_batches(self, batch_size: int) -> Iterable[np.ndarray]:\n",
        "        def gen_fn():\n",
        "            while True:\n",
        "                batch = self.read_batch(batch_size)\n",
        "                if batch is None:\n",
        "                    break\n",
        "                yield batch\n",
        "\n",
        "        rem = self.remaining()\n",
        "        num_batches = rem // batch_size + int(rem % batch_size != 0)\n",
        "        return BatchIterator(gen_fn, num_batches)\n",
        "\n",
        "\n",
        "class BatchIterator:\n",
        "    def __init__(self, gen_fn, length):\n",
        "        self.gen_fn = gen_fn\n",
        "        self.length = length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.gen_fn()\n",
        "\n",
        "\n",
        "class StreamingNpzArrayReader(NpzArrayReader):\n",
        "    def __init__(self, arr_f, shape, dtype):\n",
        "        self.arr_f = arr_f\n",
        "        self.shape = shape\n",
        "        self.dtype = dtype\n",
        "        self.idx = 0\n",
        "\n",
        "    def read_batch(self, batch_size: int) -> Optional[np.ndarray]:\n",
        "        if self.idx >= self.shape[0]:\n",
        "            return None\n",
        "\n",
        "        bs = min(batch_size, self.shape[0] - self.idx)\n",
        "        self.idx += bs\n",
        "\n",
        "        if self.dtype.itemsize == 0:\n",
        "            return np.ndarray([bs, *self.shape[1:]], dtype=self.dtype)\n",
        "\n",
        "        read_count = bs * np.prod(self.shape[1:])\n",
        "        read_size = int(read_count * self.dtype.itemsize)\n",
        "        data = _read_bytes(self.arr_f, read_size, \"array data\")\n",
        "        return np.frombuffer(data, dtype=self.dtype).reshape([bs, *self.shape[1:]])\n",
        "\n",
        "    def remaining(self) -> int:\n",
        "        return max(0, self.shape[0] - self.idx)\n"
      ],
      "metadata": {
        "id": "_nlGcAMDI00d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryNpzArrayReader(NpzArrayReader):\n",
        "    def __init__(self, arr):\n",
        "        self.arr = arr\n",
        "        self.idx = 0\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str, arr_name: str):\n",
        "        with open(path, \"rb\") as f:\n",
        "            arr = np.load(f)[arr_name]\n",
        "        return cls(arr)\n",
        "\n",
        "    def read_batch(self, batch_size: int) -> Optional[np.ndarray]:\n",
        "        if self.idx >= self.arr.shape[0]:\n",
        "            return None\n",
        "\n",
        "        res = self.arr[self.idx : self.idx + batch_size]\n",
        "        self.idx += batch_size\n",
        "        return res\n",
        "\n",
        "    def remaining(self) -> int:\n",
        "        return max(0, self.arr.shape[0] - self.idx)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def open_npz_array(path: str, arr_name: str) -> NpzArrayReader:\n",
        "    with _open_npy_file(path, arr_name) as arr_f:\n",
        "        version = np.lib.format.read_magic(arr_f)\n",
        "        if version == (1, 0):\n",
        "            header = np.lib.format.read_array_header_1_0(arr_f)\n",
        "        elif version == (2, 0):\n",
        "            header = np.lib.format.read_array_header_2_0(arr_f)\n",
        "        else:\n",
        "            yield MemoryNpzArrayReader.load(path, arr_name)\n",
        "            return\n",
        "        shape, fortran, dtype = header\n",
        "        if fortran or dtype.hasobject:\n",
        "            yield MemoryNpzArrayReader.load(path, arr_name)\n",
        "        else:\n",
        "            yield StreamingNpzArrayReader(arr_f, shape, dtype)\n",
        "\n",
        "\n",
        "def _read_bytes(fp, size, error_template=\"ran out of data\"):\n",
        "    \"\"\"\n",
        "    Copied from: https://github.com/numpy/numpy/blob/fb215c76967739268de71aa4bda55dd1b062bc2e/numpy/lib/format.py#L788-L886\n",
        "    Read from file-like object until size bytes are read.\n",
        "    Raises ValueError if not EOF is encountered before size bytes are read.\n",
        "    Non-blocking objects only supported if they derive from io objects.\n",
        "    Required as e.g. ZipExtFile in python 2.6 can return less data than\n",
        "    requested.\n",
        "    \"\"\"\n",
        "    data = bytes()\n",
        "    while True:\n",
        "        # io files (default in python3) return None or raise on\n",
        "        # would-block, python2 file will truncate, probably nothing can be\n",
        "        # done about that.  note that regular files can't be non-blocking\n",
        "        try:\n",
        "            r = fp.read(size - len(data))\n",
        "            data += r\n",
        "            if len(r) == 0 or len(data) == size:\n",
        "                break\n",
        "        except io.BlockingIOError:\n",
        "            pass\n",
        "    if len(data) != size:\n",
        "        msg = \"EOF: reading %s, expected %d bytes got %d\"\n",
        "        raise ValueError(msg % (error_template, size, len(data)))\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def _open_npy_file(path: str, arr_name: str):\n",
        "    with open(path, \"rb\") as f:\n",
        "        with zipfile.ZipFile(f, \"r\") as zip_f:\n",
        "            if f\"{arr_name}.npy\" not in zip_f.namelist():\n",
        "                raise ValueError(f\"missing {arr_name} in npz file\")\n",
        "            with zip_f.open(f\"{arr_name}.npy\", \"r\") as arr_f:\n",
        "                yield arr_f\n",
        "\n",
        "\n",
        "def _download_inception_model():\n",
        "    if os.path.exists(INCEPTION_V3_PATH):\n",
        "        return\n",
        "    print(\"downloading InceptionV3 model...\")\n",
        "    with requests.get(INCEPTION_V3_URL, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        tmp_path = INCEPTION_V3_PATH + \".tmp\"\n",
        "        with open(tmp_path, \"wb\") as f:\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=8192)):\n",
        "                f.write(chunk)\n",
        "        os.rename(tmp_path, INCEPTION_V3_PATH)\n",
        "\n",
        "\n",
        "def _create_feature_graph(input_batch):\n",
        "    _download_inception_model()\n",
        "    prefix = f\"{random.randrange(2**32)}_{random.randrange(2**32)}\"\n",
        "    with open(INCEPTION_V3_PATH, \"rb\") as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    pool3, spatial = tf.import_graph_def(\n",
        "        graph_def,\n",
        "        input_map={f\"ExpandDims:0\": input_batch},\n",
        "        return_elements=[FID_POOL_NAME, FID_SPATIAL_NAME],\n",
        "        name=prefix,\n",
        "    )\n",
        "    _update_shapes(pool3)\n",
        "    spatial = spatial[..., :7]\n",
        "    return pool3, spatial\n",
        "\n",
        "\n",
        "def _create_softmax_graph(input_batch):\n",
        "    _download_inception_model()\n",
        "    prefix = f\"{random.randrange(2**32)}_{random.randrange(2**32)}\"\n",
        "    with open(INCEPTION_V3_PATH, \"rb\") as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    (matmul,) = tf.import_graph_def(\n",
        "        graph_def, return_elements=[f\"softmax/logits/MatMul\"], name=prefix\n",
        "    )\n",
        "    w = matmul.inputs[1]\n",
        "    logits = tf.matmul(input_batch, w)\n",
        "    return tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "def _update_shapes(pool3):\n",
        "    # https://github.com/bioinf-jku/TTUR/blob/73ab375cdf952a12686d9aa7978567771084da42/fid.py#L50-L63\n",
        "    ops = pool3.graph.get_operations()\n",
        "    for op in ops:\n",
        "        for o in op.outputs:\n",
        "            shape = o.get_shape()\n",
        "            if shape._dims is not None:  # pylint: disable=protected-access\n",
        "                # shape = [s.value for s in shape] TF 1.x\n",
        "                shape = [s for s in shape]  # TF 2.x\n",
        "                new_shape = []\n",
        "                for j, s in enumerate(shape):\n",
        "                    if s == 1 and j == 0:\n",
        "                        new_shape.append(None)\n",
        "                    else:\n",
        "                        new_shape.append(s)\n",
        "                o.__dict__[\"_shape_val\"] = tf.TensorShape(new_shape)\n",
        "    return pool3\n",
        "\n",
        "\n",
        "def _numpy_partition(arr, kth, **kwargs):\n",
        "    num_workers = min(cpu_count(), len(arr))\n",
        "    chunk_size = len(arr) // num_workers\n",
        "    extra = len(arr) % num_workers\n",
        "\n",
        "    start_idx = 0\n",
        "    batches = []\n",
        "    for i in range(num_workers):\n",
        "        size = chunk_size + (1 if i < extra else 0)\n",
        "        batches.append(arr[start_idx : start_idx + size])\n",
        "        start_idx += size\n",
        "\n",
        "    with ThreadPool(num_workers) as pool:\n",
        "        return list(pool.map(partial(np.partition, kth=kth, **kwargs), batches))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChWI_0j8I025"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNs13cveI05C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUjpcmEqI07t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "GANSC1-3A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}